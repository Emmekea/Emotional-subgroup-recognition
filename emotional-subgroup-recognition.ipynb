{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f6afc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "@author: Emmeke Veltmeijer, 2022\n",
    "Notebook for finding emotional subgroups in images\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab934719",
   "metadata": {},
   "source": [
    "### Part I: Setting up ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c7b6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# workaround for https://stackoverflow.com/q/68862735/3033937\n",
    "import site; \n",
    "from os.path import join\n",
    "\n",
    "dist_package_path = site.getsitepackages()[0]\n",
    "filename = join(dist_package_path, \"keras_vggface/models.py\")\n",
    "text = open(filename).read()\n",
    "open(filename, \"w+\").write(text.replace('keras.engine.topology', 'tensorflow.keras.utils'))\n",
    "\n",
    "print(\"workaround applied\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef295e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.preprocessing import image\n",
    "from keras_vggface import utils\n",
    "import face_recognition\n",
    "import numpy as np\n",
    "import scipy\n",
    "import json\n",
    "import cv2\n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "import torchvision\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "sys.path.append('deep-head-pose-lite') \n",
    "import stable_hopenetlite\n",
    "import torch\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4143f025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading emotion recognition model\n",
    "emotion_model = keras.models.load_model('vggface_emo-recogn.h5')\n",
    "\n",
    "# Loading HopeNet light model for gaze recognition\n",
    "gaze_model = stable_hopenetlite.shufflenet_v2_x1_0()\n",
    "saved_state_dict = torch.load('deep-head-pose-lite/model/shuff_epoch_120.pkl', map_location=\"cpu\")\n",
    "gaze_model.load_state_dict(saved_state_dict, strict=False)\n",
    "gaze_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0a2cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter settings and directories\n",
    "\n",
    "# Weights assigned for emotions and coordinates of individual faces\n",
    "w_emo = 1\n",
    "w_coord = 1.5\n",
    "\n",
    "# cluster_type can be either 'kmeans' or 'hierarchical'\n",
    "cluster_type = 'kmeans'\n",
    "\n",
    "# Setting face_width and/or yaw to True will add the face size and/or yaw (gaze) to the feature vector\n",
    "face_width = False\n",
    "yaw = True\n",
    "\n",
    "# Images to test on should be placed in folder 'img'\n",
    "img_dir = './img/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ced2f2d",
   "metadata": {},
   "source": [
    "### Part II: Feature extraction ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23c3d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining functions for face recognition\n",
    "\n",
    "def face_recogn(image_full):\n",
    "    face_locations = face_recognition.face_locations(image_full)\n",
    "    return face_locations\n",
    "\n",
    "def face_matching(img_num, face_locations):\n",
    "    '''\n",
    "    Only recognized faces that match a ground truth face should be kept.\n",
    "    '''\n",
    "    \n",
    "    with open('data.json', 'r') as f:\n",
    "        full_dict = json.load(f)\n",
    "\n",
    "    temp_dict = {}\n",
    "\n",
    "    # loop over the detections\n",
    "    for idx, face in enumerate(face_locations):\n",
    "        \n",
    "        # compute the coordinates of the bounding box for the object\n",
    "        startX = int(face[3])\n",
    "        startY = int(face[0])\n",
    "        endX = int(face[1])\n",
    "        endY = int(face[2])\n",
    "\n",
    "        temp_dict[idx] = {'bbox_x': [startX, endX], 'bbox_y': [startY, endY]}\n",
    "                  \n",
    "    face_numbers = []\n",
    "    for face in full_dict[img_num][\"faces\"]:\n",
    "        face_numbers.append(int(face))\n",
    "    \n",
    "    face_locations_matched = [0] * (max(face_numbers)+1) #reserve a spot for all ground truth faces\n",
    "      \n",
    "    #looking for the matching mask\n",
    "    for mask in full_dict[img_num][\"faces\"]:\n",
    "        for detected_mask in temp_dict:\n",
    "            largest_x_idx = np.argmax([temp_dict[detected_mask]['bbox_x'][1],full_dict[str(img_num)][\"faces\"][mask]['bbox_x'][1]])\n",
    "            smallest_x_idx = np.argmin([temp_dict[detected_mask]['bbox_x'][0],full_dict[str(img_num)][\"faces\"][mask]['bbox_x'][0]])\n",
    "            \n",
    "            # no overlap\n",
    "            if largest_x_idx == 1 and smallest_x_idx == 0 and full_dict[str(img_num)][\"faces\"][mask]['bbox_x'][0] >= temp_dict[detected_mask]['bbox_x'][1]:\n",
    "                x_overlap = 0\n",
    "                x_union = 1\n",
    "            elif largest_x_idx == 0 and smallest_x_idx == 1 and temp_dict[detected_mask]['bbox_x'][0] >= full_dict[str(img_num)][\"faces\"][mask]['bbox_x'][1]:\n",
    "                x_overlap = 0\n",
    "                x_union = 1\n",
    "            \n",
    "            # some overlap\n",
    "            elif largest_x_idx == 0 and smallest_x_idx == 0:\n",
    "                x_overlap = full_dict[str(img_num)][\"faces\"][mask]['bbox_x'][1] - full_dict[str(img_num)][\"faces\"][mask]['bbox_x'][0]\n",
    "                x_union = temp_dict[detected_mask]['bbox_x'][1] - temp_dict[detected_mask]['bbox_x'][0]\n",
    "            elif largest_x_idx == 0 and smallest_x_idx == 1:\n",
    "                x_overlap = full_dict[str(img_num)][\"faces\"][mask]['bbox_x'][1] - temp_dict[detected_mask]['bbox_x'][0]\n",
    "                x_union = temp_dict[detected_mask]['bbox_x'][1] - full_dict[str(img_num)][\"faces\"][mask]['bbox_x'][0]\n",
    "            elif largest_x_idx == 1 and smallest_x_idx == 0:\n",
    "                x_overlap = temp_dict[detected_mask]['bbox_x'][1] - full_dict[str(img_num)][\"faces\"][mask]['bbox_x'][0]\n",
    "                x_union = full_dict[str(img_num)][\"faces\"][mask]['bbox_x'][1] - temp_dict[detected_mask]['bbox_x'][0]\n",
    "            elif largest_x_idx == 1 and smallest_x_idx == 1:\n",
    "                x_overlap = temp_dict[detected_mask]['bbox_x'][1] - temp_dict[detected_mask]['bbox_x'][0]\n",
    "                x_union = full_dict[str(img_num)][\"faces\"][mask]['bbox_x'][1] - full_dict[str(img_num)][\"faces\"][mask]['bbox_x'][0]\n",
    "\n",
    "            IoU_x = x_overlap / x_union\n",
    "\n",
    "            largest_y_idx = np.argmax([temp_dict[detected_mask]['bbox_y'][1],full_dict[str(img_num)][\"faces\"][mask]['bbox_y'][1]])\n",
    "            smallest_y_idx = np.argmin([temp_dict[detected_mask]['bbox_y'][0],full_dict[str(img_num)][\"faces\"][mask]['bbox_y'][0]])\n",
    "            \n",
    "            # no overlap\n",
    "            if largest_y_idx == 1 and smallest_y_idx == 0 and full_dict[str(img_num)][\"faces\"][mask]['bbox_y'][0] >= temp_dict[detected_mask]['bbox_x'][1]:\n",
    "                y_overlap = 0\n",
    "                y_union = 1\n",
    "            elif largest_y_idx == 0 and smallest_y_idx == 1 and temp_dict[detected_mask]['bbox_y'][0] >= full_dict[str(img_num)][\"faces\"][mask]['bbox_y'][1]:\n",
    "                y_overlap = 0\n",
    "                y_union = 1\n",
    "            \n",
    "            # some overlap\n",
    "            elif largest_y_idx == 0 and smallest_y_idx == 0:\n",
    "                y_overlap = full_dict[str(img_num)][\"faces\"][mask]['bbox_y'][1] - full_dict[str(img_num)][\"faces\"][mask]['bbox_y'][0]\n",
    "                y_union = temp_dict[detected_mask]['bbox_y'][1] - temp_dict[detected_mask]['bbox_y'][0]\n",
    "            elif largest_y_idx == 0 and smallest_y_idx == 1:\n",
    "                y_overlap = full_dict[str(img_num)][\"faces\"][mask]['bbox_y'][1] - temp_dict[detected_mask]['bbox_y'][0]\n",
    "                y_union = temp_dict[detected_mask]['bbox_y'][1] - full_dict[str(img_num)][\"faces\"][mask]['bbox_y'][0]\n",
    "            elif largest_y_idx == 1 and smallest_y_idx == 0:\n",
    "                y_overlap = temp_dict[detected_mask]['bbox_y'][1] - full_dict[str(img_num)][\"faces\"][mask]['bbox_y'][0]\n",
    "                y_union = full_dict[str(img_num)][\"faces\"][mask]['bbox_y'][1] - temp_dict[detected_mask]['bbox_y'][0]\n",
    "            elif largest_y_idx == 1 and smallest_y_idx == 1:\n",
    "                y_overlap = temp_dict[detected_mask]['bbox_y'][1] - temp_dict[detected_mask]['bbox_y'][0]\n",
    "                y_union = full_dict[str(img_num)][\"faces\"][mask]['bbox_y'][1] - full_dict[str(img_num)][\"faces\"][mask]['bbox_y'][0]\n",
    "\n",
    "            IoU_y = y_overlap / y_union\n",
    "\n",
    "            #we require an IoU of at least 0.3\n",
    "            if 0.2 <= IoU_x <= 1.0 and 0.2 <= IoU_y <= 1.0:\n",
    "                face_locations_matched[int(mask)]= face_locations[detected_mask]\n",
    "                break\n",
    "         \n",
    "    to_remove = []\n",
    "    for idx, listornot in enumerate(face_locations_matched):\n",
    "        # if face idx was not recognized, it should be removed from the ground truth list\n",
    "        if type(listornot) == int: #still initialized 0 instead of list with coordinates\n",
    "            to_remove.append(idx)\n",
    "\n",
    "    for item in sorted(to_remove, reverse=True):\n",
    "        del face_locations_matched[item]\n",
    "                            \n",
    "    return face_locations_matched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f862e0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining function for individual emotion recognition\n",
    "\n",
    "def emotion_recognition(image_full, idx, face):\n",
    "\n",
    "    # compute the coordinates of the bounding box for the object\n",
    "    startX = int(face[3])\n",
    "    startY = int(face[0])\n",
    "    endX = int(face[1])\n",
    "    endY = int(face[2])\n",
    "\n",
    "    center_face_x = startX + (endX - startX) / 2\n",
    "    center_face_y = startY + (endY - startY) / 2\n",
    "    x_norm = center_face_x / len(image_full[1]) #normalize\n",
    "    y_norm = center_face_y / len(image_full[0]) #normalize\n",
    "    radius_x = endX - startX\n",
    "    radius_y = endY - startY\n",
    "\n",
    "    img_cropped = image_full[startY:endY,startX:endX]\n",
    "    img_resized = cv2.resize(img_cropped,(224, 224))\n",
    "\n",
    "    x = keras.utils.img_to_array(img_resized)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = utils.preprocess_input(x, version=1)\n",
    "    predictions = emotion_model.predict(x)\n",
    "    predictions_valence = [predictions[0][0]+predictions[0][1]+predictions[0][2]+predictions[0][5],predictions[0][4],predictions[0][3]]\n",
    "    predicted_classes = np.argmax(predictions_valence)\n",
    "\n",
    "    return predictions_valence, center_face_x, center_face_y, x_norm, y_norm, radius_x, radius_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e89263c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining functions for extracting and saving individual information\n",
    "\n",
    "def information_extraction(img_num, img_path, cluster_output):\n",
    "#     start csv files with k-means data\n",
    "    new_filename = './temp/coordinates_emotion_output_img' + str(img_num) +'.csv'\n",
    "    \n",
    "    with open(new_filename, mode='w') as coordinates_file:\n",
    "        coordinates_writer = csv.writer(coordinates_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "        coordinates_writer.writerow(['conf_0','conf_1','conf_2','x_cor', 'y_cor'])\n",
    "    \n",
    "    image_full = face_recognition.load_image_file(img_path)\n",
    "    face_locations_unmatched = face_recogn(image_full)\n",
    "    face_locations = face_matching(img_num, face_locations_unmatched)\n",
    "    \n",
    "    cluster_output[img_num] = {}\n",
    "    \n",
    "    for idx, face in enumerate(face_locations):\n",
    "        predictions_valence, center_face_x, center_face_y, x_norm, y_norm, radius_x, radius_y = emotion_recognition(image_full, idx, face)\n",
    "\n",
    "        cluster_output[img_num][idx] = {}\n",
    "\n",
    "        dict_addition = {'conf_0':float(predictions_valence[0]), 'conf_1':float(predictions_valence[1]), 'conf_2':float(predictions_valence[2]), 'x_cor':center_face_x, 'y_cor':center_face_y, 'x_norm':x_norm, 'y_norm':y_norm, 'radius_x':radius_x, 'radius_y':radius_y}\n",
    "        cluster_output[img_num][idx].update(dict_addition)\n",
    "\n",
    "        with open(new_filename, mode='a') as coordinates_file:\n",
    "            coordinates_writer = csv.writer(coordinates_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "            coordinates_writer.writerow([predictions_valence[0],predictions_valence[1],predictions_valence[2],x_norm,y_norm])\n",
    "      \n",
    "    return cluster_output\n",
    "\n",
    "def gaze_extraction(img_num, img_path, cluster_output):\n",
    "\n",
    "    img_path = os.path.join(img_dir,img_name)\n",
    "    \n",
    "    for face_name in cluster_output[img_num]:\n",
    "        centerX = cluster_output[img_num][face_name]['x_cor']\n",
    "        centerY = cluster_output[img_num][face_name]['y_cor']\n",
    "        radiusX = cluster_output[img_num][face_name]['radius_x']\n",
    "        radiusY = cluster_output[img_num][face_name]['radius_y']\n",
    "        startY = centerY - (radiusY/2)\n",
    "        endY = startY + radiusY\n",
    "        startX = centerX - (radiusX/2)\n",
    "        endX = startX + radiusX\n",
    "\n",
    "        img = Image.open(img_path)\n",
    "        img = img.convert('RGB')\n",
    "        img = img.crop((startX, startY, endX, endY))\n",
    "\n",
    "        transformations = transforms.Compose([transforms.Scale(224),\n",
    "        transforms.CenterCrop(224), transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "        img = transformations(img)\n",
    "        img_shape = img.size()\n",
    "        img = img.view(1, img_shape[0], img_shape[1], img_shape[2])\n",
    "\n",
    "        yaw, pitch, roll = gaze_model(img)\n",
    "\n",
    "        yaw_predicted = F.softmax(yaw)\n",
    "        pitch_predicted = F.softmax(pitch)\n",
    "        roll_predicted = F.softmax(roll)\n",
    "\n",
    "        # Get continuous predictions in degrees.\n",
    "        idx_tensor = [idx for idx in range(66)]\n",
    "        idx_tensor = torch.FloatTensor(idx_tensor)\n",
    "\n",
    "        yaw_predicted = torch.sum(yaw_predicted.data[0] * idx_tensor) * 3 - 99\n",
    "\n",
    "        yaw_direction_norm = yaw_predicted / 45\n",
    "            \n",
    "        cluster_output[img_num][face_name]['yaw_norm'] = yaw_direction_norm.item() #.item(): convert tensor to float\n",
    "        \n",
    "    return cluster_output\n",
    "\n",
    "def face_width_extraction(img_num, cluster_output):\n",
    "    \n",
    "    all_widths = []\n",
    "\n",
    "    for faces in cluster_output[img_num]: #find biggest face\n",
    "        all_widths.append(cluster_output[img_num][faces]['radius_x'])\n",
    "\n",
    "    for faces in cluster_output[img_num]: #normalize\n",
    "        cluster_output[img_num][faces]['face_width'] = cluster_output[img_num][faces]['radius_x'] / max(all_widths)\n",
    "        \n",
    "    return cluster_output\n",
    "\n",
    "def emotion_assignment(img_num, K, cluster_output, cluster_type):\n",
    "\n",
    "    for i in range(K): #looping through number of clusters\n",
    "        prob_sum = [0,0,0]\n",
    "\n",
    "        if cluster_type == 'kmeans':\n",
    "            for idx in range(len(cluster_output[img_num])):\n",
    "                if cluster_output[img_num][str(idx)]['kmeans_cluster'] == i:\n",
    "                    prob_sum[0] += cluster_output[img_num][str(idx)]['conf_0']\n",
    "                    prob_sum[1] += cluster_output[img_num][str(idx)]['conf_1']\n",
    "                    prob_sum[2] += cluster_output[img_num][str(idx)]['conf_2']\n",
    "            predicted_class = np.argmax(prob_sum)\n",
    "\n",
    "            for idx in range(len(cluster_output[img_num])):\n",
    "                if cluster_output[img_num][str(idx)]['kmeans_cluster'] == i:\n",
    "                    cluster_output[img_num][str(idx)]['kmeans_emotion'] = int(predicted_class)\n",
    "                    \n",
    "        elif cluster_type == 'hierarchical':\n",
    "            for idx in range(len(cluster_output[img_num])):\n",
    "                if cluster_output[img_num][str(idx)]['hierarchical_cluster'] == i:\n",
    "                    prob_sum[0] += cluster_output[img_num][str(idx)]['conf_0']\n",
    "                    prob_sum[1] += cluster_output[img_num][str(idx)]['conf_1']\n",
    "                    prob_sum[2] += cluster_output[img_num][str(idx)]['conf_2']\n",
    "            predicted_class = np.argmax(prob_sum)\n",
    "            \n",
    "            for idx in range(len(cluster_output[img_num])):\n",
    "                if cluster_output[img_num][str(idx)]['hierarchical_cluster'] == i:\n",
    "                    cluster_output[img_num][str(idx)]['hierarchical_emotion'] = int(predicted_class)\n",
    "                \n",
    "    return cluster_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1c5899",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Extracting and saving features\n",
    "\n",
    "cluster_output = {}\n",
    "\n",
    "for count, img_name in enumerate(os.listdir(img_dir)):\n",
    "    img_num = img_name[:-4]\n",
    "    img_path = os.path.join(img_dir,img_name)\n",
    "\n",
    "    cluster_output = information_extraction(img_num, img_path, cluster_output)\n",
    "    \n",
    "    if count % 10 == 0:\n",
    "        print(f'Finished baseline feature extraction from image {count}')\n",
    "    \n",
    "if face_width:\n",
    "    for count, img_name in enumerate(os.listdir(img_dir)):\n",
    "        img_num = img_name[:-4]\n",
    "        img_path = os.path.join(img_dir,img_name)\n",
    "\n",
    "        cluster_output = face_width_extraction(img_num, cluster_output)\n",
    "\n",
    "        if count % 10 == 0:\n",
    "            print(f'Finished additional face_width feature extraction from image {count}')\n",
    "    \n",
    "if yaw:\n",
    "    for count, img_name in enumerate(os.listdir(img_dir)):\n",
    "        img_num = img_name[:-4]\n",
    "        img_path = os.path.join(img_dir,img_name)\n",
    "\n",
    "        cluster_output = gaze_extraction(img_num, img_path, cluster_output)\n",
    "\n",
    "        if count % 10 == 0:\n",
    "            print(f'Finished additional gaze feature extraction from image {count}')\n",
    "            \n",
    "with open('./output_files/image_information.json', 'w') as f:\n",
    "    json.dump(cluster_output, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c636c838",
   "metadata": {},
   "source": [
    "### Part III: Emotional subgroup clustering ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f26b24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining functions for k-means clustering\n",
    "\n",
    "def optimal_k(X):\n",
    "    '''\n",
    "    Method from https://www.datasciencecentral.com/profiles/blogs/how-to-automatically-determine-the-number-of-clusters-in-your-dat\n",
    "    Accessed on [16-06-2021]\n",
    "    '''\n",
    "    k_range = range(1, len(X)+1)\n",
    "    km = [KMeans(n_clusters=i, random_state=0) for i in k_range]\n",
    "    scores = [km[i].fit(X).score(X) for i in range(len(km))]\n",
    "    \n",
    "    if scores[0] < 0:\n",
    "        for num,i in enumerate(scores):\n",
    "            scores[num] = -i\n",
    "    \n",
    "    deltas_1 = np.zeros(len(scores))\n",
    "    for i in range(1,len(deltas_1)):\n",
    "        deltas_1[i] = scores[i-1]-scores[i]\n",
    "\n",
    "    deltas_2 = np.zeros(len(deltas_1))\n",
    "    for i in range(1,len(deltas_2)):\n",
    "        deltas_2[i] = deltas_1[i-1]-deltas_1[i]\n",
    "\n",
    "    strength = np.zeros(len(deltas_2))\n",
    "    for i in range(1,len(strength)-1):\n",
    "        strength[i] = deltas_2[i+1]-deltas_1[i+1]\n",
    "\n",
    "    max_value = np.max(strength)\n",
    "    optimal_k = np.where(strength==max_value)[0][0] + 1 # correcting for index number\n",
    "    \n",
    "    return k_range, scores, optimal_k\n",
    "\n",
    "def kmeans_regular(img_num, cluster_output, face_width, yaw):\n",
    "    \n",
    "    current_filename = './temp/coordinates_emotion_output_img' + str(img_num) +'.csv'\n",
    "    \n",
    "    train = pd.read_csv(current_filename)\n",
    "    X = np.array(train.astype(float))\n",
    "    \n",
    "    # weighing emotion and distance importance\n",
    "    for row in X:\n",
    "        row[0] = row[0] * w_emo\n",
    "        row[1] = row[1] * w_emo\n",
    "        row[2] = row[2] * w_emo\n",
    "        row[3] = row[3] * w_coord\n",
    "        row[4] = row[4] * w_coord\n",
    "    \n",
    "    if face_width: # if face size should be included in the feature vector\n",
    "        face_widths = np.zeros((X.shape[0],1))\n",
    "\n",
    "        for idx,face in enumerate(cluster_output[img_num]):\n",
    "            face_widths[idx] = cluster_output[img_num][face]['face_width']\n",
    "\n",
    "        X = np.append(X, face_widths, axis=1)\n",
    "        \n",
    "    if yaw: # if yaw should be included in the feature vector\n",
    "        yaws = np.zeros((X.shape[0],1))\n",
    "\n",
    "        for idx,face in enumerate(cluster_output[img_num]):\n",
    "            yaws[idx] = cluster_output[img_num][face]['yaw_norm']\n",
    "\n",
    "        X = np.append(X, yaws, axis=1)\n",
    "    \n",
    "    _, _, k_opt = optimal_k(X)\n",
    "    kmeans = KMeans(n_clusters=k_opt, random_state=0).fit(X)\n",
    "    \n",
    "    #saving k-means labels\n",
    "    for idx in range(len(cluster_output[img_num])):\n",
    "        cluster_output[img_num][str(idx)]['kmeans_cluster'] = int(kmeans.labels_[idx])\n",
    "    \n",
    "    return k_opt, cluster_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605368c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining functions for hierarchical clustering\n",
    "\n",
    "class hierarchical_clustering:\n",
    "    def __init__(self,data):\n",
    "        self.data = data\n",
    "        self.centroids = {idx:row for idx, row in enumerate(data)}\n",
    "        self.distances = {}\n",
    "        self.clusters = {x:[x] for x in range(np.shape(data)[0])}\n",
    "        self.flat_clusters = {}\n",
    "        self.cluster_history = {}\n",
    "        self.distance_history = []\n",
    "        self.current_clusters = [x for x in range(np.shape(data)[0])]\n",
    "        \n",
    "        for i in range(np.shape(data)[0]):\n",
    "            for j in range(i+1,np.shape(data)[0]):\n",
    "                dist = np.linalg.norm(data[i,:]-data[j,:])\n",
    "                self.distances[i,j] = dist\n",
    "                \n",
    "    def clustering(self, k):\n",
    "        \n",
    "        while len(self.current_clusters) > k:\n",
    "            distances_ordered = sorted(self.distances.items(), key=lambda item: item[1])\n",
    "            min_pair = distances_ordered[0]\n",
    "            new_cluster_members = min_pair[0]\n",
    "            min_dist = min_pair[1]\n",
    "\n",
    "            # updating the list of current clusters\n",
    "            self.current_clusters.remove(new_cluster_members[0])\n",
    "            self.current_clusters.remove(new_cluster_members[1])\n",
    "            new_cluster_id = len(self.clusters) #this is always 1 more than the current highest number\n",
    "            self.current_clusters.append(new_cluster_id) \n",
    "\n",
    "            # updating the cluster history: keeps track of all clusters with their distances\n",
    "            self.cluster_history[new_cluster_members] = min_dist\n",
    "            self.distance_history.append(min_dist)\n",
    "\n",
    "            # updating the clusters: keeps track which entities belong to which cluster IDs. Nothing is deleted for\n",
    "            # the purpose of retrieving which cluster was formed by which points/clusters\n",
    "            self.clusters[new_cluster_id] = new_cluster_members\n",
    "            flat_new_members = list(new_cluster_members)\n",
    "            if new_cluster_members[0] in self.flat_clusters:\n",
    "                flat_new_members[0] = list(self.flat_clusters[new_cluster_members[0]])\n",
    "            if new_cluster_members[1] in self.flat_clusters:\n",
    "                flat_new_members[1] = list(self.flat_clusters[new_cluster_members[1]])\n",
    "                    \n",
    "            for member in flat_new_members:\n",
    "                if new_cluster_id in self.flat_clusters:\n",
    "                    if type(member) == int:\n",
    "                        self.flat_clusters[new_cluster_id].extend([member])\n",
    "                    elif type(member) == list:\n",
    "                        self.flat_clusters[new_cluster_id].extend(member)\n",
    "                else: #first value\n",
    "                    if type(member) == int:\n",
    "                        self.flat_clusters[new_cluster_id] = [member]\n",
    "                    elif type(member) == list:\n",
    "                        self.flat_clusters[new_cluster_id] = member\n",
    "\n",
    "            # updating the centroids: keeps track of the centroids of all clusters. Individual values are not deleted\n",
    "            # since their vectors need to be preserved. Clusters of > 1 point are removed once they merge with\n",
    "            # with another point/cluster.\n",
    "            \n",
    "            new_centroid = self.calculating_centroids(new_cluster_members)\n",
    "            if len(new_cluster_members) > 2:\n",
    "                if len(new_cluster_members[0]) > 1: del self.centroids[new_cluster_members[0]]\n",
    "                if len(new_cluster_members[1]) > 1: del self.centroids[new_cluster_members[1]]\n",
    "\n",
    "            self.centroids[new_cluster_id] = new_centroid     \n",
    "\n",
    "            # updating the distances: keeps track of distances between clusters\n",
    "            to_delete = []\n",
    "            for key, value in self.distances.items():\n",
    "                if new_cluster_members[0] in key or new_cluster_members[1] in key:\n",
    "                    to_delete.append(key)\n",
    "            for key in to_delete:\n",
    "                del self.distances[key]\n",
    "            self.calculating_distances(new_cluster_id, self.current_clusters)\n",
    "            \n",
    "            # generate output in a readable format\n",
    "            output = [None] * np.shape(data)[0]\n",
    "            for i in range(np.shape(data)[0]): \n",
    "                exhausted = False\n",
    "                if i in self.current_clusters:\n",
    "                    output[i] = self.current_clusters.index(i)\n",
    "                else:\n",
    "                    j = i\n",
    "                    while exhausted == False:\n",
    "                        for key, value in self.clusters.items():\n",
    "                            if j in value and j != key:\n",
    "                                if key in self.current_clusters:\n",
    "                                    output[i] = self.current_clusters.index(key)\n",
    "                                    exhausted = True\n",
    "                                else:\n",
    "                                    j = key\n",
    "          \n",
    "        try:\n",
    "            return output, self.current_clusters, self.clusters, self.cluster_history, self.distance_history\n",
    "        except:\n",
    "            print(f'Failed to return any output for image {img_num}')\n",
    "        \n",
    "    def calculating_centroids(self, new_cluster_members):\n",
    "        # for every new cluster the centroid should be calculated\n",
    "        \n",
    "        sum_centroids = 0\n",
    "        total_len = 0\n",
    "        for i in new_cluster_members:\n",
    "            if i in self.flat_clusters:\n",
    "                for j in self.flat_clusters[i]: # retrieving all original entities in cluster\n",
    "                    sum_centroids += self.centroids[j]\n",
    "                    total_len += 1\n",
    "            else: # cluster consists of a single entity\n",
    "                sum_centroids += self.centroids[i]\n",
    "                total_len += 1\n",
    "                \n",
    "        new_centroid = sum_centroids / total_len\n",
    "        \n",
    "        return new_centroid\n",
    "        \n",
    "    def calculating_distances(self, new_cluster_id, current_clusters):\n",
    "        # calculating euclidean distance between every two clusters\n",
    "        \n",
    "        for cluster in self.current_clusters:\n",
    "            if cluster != new_cluster_id:\n",
    "                dist = np.linalg.norm(self.centroids[new_cluster_id] - self.centroids[cluster])\n",
    "                self.distances[new_cluster_id,cluster] = dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2e3b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hierarchical_k_opt(distance_history):\n",
    "    relative_distances = distance_history\n",
    "    for i in range(len(distance_history)):\n",
    "        if i > 0:\n",
    "            relative_distances[i] = relative_distances[i] - relative_distances[i-1]\n",
    "\n",
    "    max_dist_index = relative_distances.index(max(relative_distances))\n",
    "    optimal_k = len(relative_distances) - max_dist_index + 1 # +1 because you want the number of clusters before the maximum distance\n",
    "    \n",
    "    return optimal_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17e5892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering and saving output\n",
    "\n",
    "cluster_output = {}\n",
    "\n",
    "with open('./output_files/image_information.json', 'r') as f:\n",
    "    cluster_output = json.load(f)\n",
    "\n",
    "if cluster_type == 'kmeans':\n",
    "    for count, img_name in enumerate(os.listdir(img_dir)):\n",
    "        img_num = img_name[:-4]\n",
    "        K, cluster_output = kmeans_regular(img_num=img_num, cluster_output=cluster_output, face_width=face_width, yaw=yaw)\n",
    "        cluster_output = emotion_assignment(img_num, K, cluster_output, 'kmeans')\n",
    "\n",
    "        if count % 10 == 0:\n",
    "            print(f'Finished clustering image {count}')\n",
    "\n",
    "    with open('./output_files/image_information.json', 'w') as f:\n",
    "        json.dump(cluster_output, f)\n",
    "            \n",
    "if cluster_type == 'hierarchical':\n",
    "    if __name__ == '__main__':\n",
    "\n",
    "        for count, img_name in enumerate(os.listdir(img_dir)):\n",
    "            img_num = img_name[:-4]\n",
    "            current_filename = './temp/coordinates_emotion_output_img' + str(img_num) +'.csv'\n",
    "            data = pd.read_csv(current_filename)\n",
    "            data = np.array(data.astype(float))\n",
    "\n",
    "            # weighing emotion and distance importance\n",
    "            for row in data:\n",
    "                row[0] = row[0] * w_emo\n",
    "                row[1] = row[1] * w_emo\n",
    "                row[2] = row[2] * w_emo\n",
    "                row[3] = row[3] * w_coord\n",
    "                row[4] = row[4] * w_coord\n",
    "\n",
    "            if face_width: # if face size should be included in the feature vector\n",
    "                face_widths = np.zeros((data.shape[0],1))\n",
    "\n",
    "                for idee,face in enumerate(cluster_output[img_num]):\n",
    "                    face_widths[idee] = cluster_output[img_num][face]['face_width']\n",
    "\n",
    "                data = np.append(data, face_widths, axis=1)\n",
    "\n",
    "            if yaw: # if yaw should be included in the feature vector\n",
    "                yaws = np.zeros((data.shape[0],1))\n",
    "\n",
    "                for idee,face in enumerate(cluster_output[img_num]):\n",
    "                    yaws[idee] = cluster_output[img_num][face]['yaw_norm']\n",
    "\n",
    "                data = np.append(data, yaws, axis=1)\n",
    "\n",
    "            K = 1\n",
    "\n",
    "            hc = hierarchical_clustering(data)\n",
    "            output, current_clusters, clusters, cluster_history, distance_history = hc.clustering(K)\n",
    "\n",
    "            #find optimal k\n",
    "            K = hierarchical_k_opt(distance_history)\n",
    "\n",
    "            if K < len(cluster_output[str(img_num)]):\n",
    "                hc = hierarchical_clustering(data)\n",
    "                output, current_clusters, clusters, cluster_history, distance_history = hc.clustering(K)\n",
    "\n",
    "            elif K == len(cluster_output[str(img_num)]):\n",
    "                output = list(range(K))\n",
    "\n",
    "            #saving hierarchical cluster labels\n",
    "            for idxx in range(len(cluster_output[img_num])):\n",
    "                cluster_output[img_num][str(idxx)]['hierarchical_cluster'] = int(output[idxx])\n",
    "\n",
    "            cluster_output = emotion_assignment(img_num, K, cluster_output, cluster_type)\n",
    "\n",
    "            if count % 10 == 0:\n",
    "                print(f'Finished clustering image {count}')\n",
    "\n",
    "        #saving json dict\n",
    "        with open('./output_files/image_information.json', 'w') as f:\n",
    "            json.dump(cluster_output, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fcda135",
   "metadata": {},
   "source": [
    "### Part IV: Evaluating emotional subgroup classification  ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549d632e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The following code will compare the acquired clustering results against a provided ground truth.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f1f8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./output_files/image_information.json', 'r') as f:\n",
    "    clustering_output = json.load(f)\n",
    "with open('data.json', 'r') as f:\n",
    "    ground_truth = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e003819b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining functions for calculating the Hamming distance (subgroup part of testing)\n",
    "\n",
    "def get_group_idx(groups,face_id_i):\n",
    "    for idx, group in enumerate(groups):\n",
    "        if face_id_i in group:\n",
    "            return idx\n",
    "    return -1\n",
    "\n",
    "def build_adjacency_matrix_gt(ground_truth, img_num, num_faces, adj_matrix):\n",
    "\n",
    "    groups = ground_truth[img_num]['emotional_subgroup_labels']['groups']\n",
    "    link_dict = {}\n",
    "    for face_id_i in range(num_faces):\n",
    "        group_idx = get_group_idx(groups,str(face_id_i))\n",
    "        link_dict[face_id_i] = group_idx\n",
    "\n",
    "    for face_id in link_dict:\n",
    "        not_linked_yet = True\n",
    "        for other_face in link_dict:\n",
    "            if face_id != other_face:\n",
    "                if link_dict[face_id] == link_dict[other_face]:\n",
    "                    adj_matrix[face_id,other_face] = 1\n",
    "                    not_linked_yet = False\n",
    "        if not_linked_yet: #alone in a group thus linked to itself\n",
    "            adj_matrix[face_id,face_id] = 1\n",
    "        \n",
    "    return adj_matrix\n",
    "\n",
    "def build_adjacency_matrix(clustering_output, img_num, num_faces, adj_matrix):\n",
    "\n",
    "    for face_id_i in range(num_faces):\n",
    "        not_linked_yet = True\n",
    "        for other_face in range(num_faces):\n",
    "            if face_id_i != other_face:\n",
    "                if cluster_type == 'kmeans':\n",
    "                    if clustering_output[img_num][str(face_id_i)]['kmeans_cluster'] == clustering_output[img_num][str(other_face)]['kmeans_cluster']:\n",
    "                        adj_matrix[face_id_i,other_face] = 1\n",
    "                        not_linked_yet = False\n",
    "                elif cluster_type == 'hierarchical':\n",
    "                    if clustering_output[img_num][str(face_id_i)]['hierarchical_cluster'] == clustering_output[img_num][str(other_face)]['hierarchical_cluster']:\n",
    "                        adj_matrix[face_id_i,other_face] = 1\n",
    "                        not_linked_yet = False\n",
    "        if not_linked_yet: #alone in a group thus linked to itself\n",
    "            adj_matrix[face_id_i,face_id_i] = 1\n",
    "    \n",
    "    return adj_matrix\n",
    "\n",
    "def hamming_distance(u,v):\n",
    "    h_dist = 0\n",
    "    for idx, row in enumerate(u):\n",
    "        h_dist += scipy.spatial.distance.hamming(row, v[idx], w=None)\n",
    "    h_dist = h_dist / len(u)\n",
    "    return h_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef199124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the Hamming distance (subgroup part of testing)\n",
    "\n",
    "h_dist_cl = []\n",
    "\n",
    "for img_name in os.listdir(img_dir):\n",
    "    img_path = os.path.join(img_dir,img_name)\n",
    "    img_num = img_name[:-4]\n",
    "\n",
    "    if len(ground_truth[img_num]['emotional_subgroup_labels']['groups']) > 0:\n",
    "\n",
    "        # First, count the number of faces in this image\n",
    "        num_faces = len(clustering_output[img_num])\n",
    "\n",
    "        # Construct adjacency matrix for both ground truth and cluster output\n",
    "        adj_matrix_gt = np.zeros((num_faces,num_faces))\n",
    "        adj_matrix_gt = build_adjacency_matrix_gt(ground_truth, img_num, num_faces, adj_matrix_gt)\n",
    "\n",
    "        adj_matrix_cl = np.zeros((num_faces,num_faces))\n",
    "        adj_matrix_cl = build_adjacency_matrix(clustering_output, img_num, num_faces, adj_matrix_cl)\n",
    "\n",
    "        # Calculate Hamming distance per row, then per matrix, then per dataset\n",
    "        h_dist = hamming_distance(adj_matrix_gt,adj_matrix_cl)\n",
    "        h_dist_cl.append(h_dist)\n",
    "\n",
    "print(f'Out of {len(h_dist_cl)} approved images, the average h_dist for {cluster_type} clustering is {np.mean(h_dist_cl)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c33f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining function for calculating the accuracy (emotion part of testing)\n",
    "\n",
    "def construct_emotional_dict(img_num, num_faces):\n",
    "    emotional_dict = {}\n",
    "    groups = ground_truth[img_num]['emotional_subgroup_labels']['groups']\n",
    "    for face_id_i in range(num_faces):\n",
    "        group_idx = get_group_idx(groups,str(face_id_i))\n",
    "        emotional_dict[face_id_i] = ground_truth[img_num]['emotional_subgroup_labels']['emotions'][group_idx]\n",
    "    return emotional_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8f4ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the accuracy (emotion part of testing)\n",
    "\n",
    "acc = []\n",
    "\n",
    "pred = {0: {0: 0, 1: 0, 2:0}, 1: {0: 0, 1: 0, 2:0}, 2: {0: 0, 1: 0, 2:0}} \n",
    "# {true_neg: {pred_neg: 0, pred_neu: 0, pred_pos: 0}, \n",
    "# true_neu: {pred_neg: 0, pred_neu: 0, pred_pos: 0}, \n",
    "# true_pos: {pred_neg: 0, pred_neu: 0, pred_pos: 0}}\n",
    "\n",
    "for img_name in os.listdir(img_dir):\n",
    "    img_path = os.path.join(img_dir,img_name)\n",
    "    img_num = img_name[:-4]\n",
    "\n",
    "    if len(ground_truth[img_num]['emotional_subgroup_labels']['groups']) > 0:\n",
    "\n",
    "        num_faces = len(clustering_output[img_num])\n",
    "        emotional_dict = construct_emotional_dict(img_num, num_faces)\n",
    "\n",
    "        correct = 0\n",
    "        incorrect = 0\n",
    "\n",
    "        for face_id_i in range(num_faces):\n",
    "            if cluster_type == 'kmeans':\n",
    "                pred[emotional_dict[face_id_i]][clustering_output[img_num][str(face_id_i)]['kmeans_emotion']] += 1\n",
    "                if emotional_dict[face_id_i] == clustering_output[img_num][str(face_id_i)]['kmeans_emotion']:\n",
    "                    correct += 1\n",
    "                elif emotional_dict[face_id_i] != clustering_output[img_num][str(face_id_i)]['kmeans_emotion']:\n",
    "                    incorrect += 1\n",
    "            \n",
    "            if cluster_type == 'hierarchical':\n",
    "                pred[emotional_dict[face_id_i]][clustering_output[img_num][str(face_id_i)]['hierarchical_emotion']] += 1\n",
    "                if emotional_dict[face_id_i] == clustering_output[img_num][str(face_id_i)]['hierarchical_emotion']:\n",
    "                    correct += 1\n",
    "                elif emotional_dict[face_id_i] != clustering_output[img_num][str(face_id_i)]['hierarchical_emotion']:\n",
    "                    incorrect += 1\n",
    "\n",
    "        acc.append(correct/(correct+incorrect))\n",
    "\n",
    "print(f'Out of {len(acc)} approved images, the average acc for {cluster_type} clustering is {np.mean(acc)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6e31cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For further analysis of the accuracy outcome, e.g. in case of imbalanced datasets when the accuracy \n",
    "# score can be misleading, the true/false positives and true/false negatives are printed in a contingency table.\n",
    "\n",
    "metric_dict = {'true_negative': [pred[0][0], pred[0][1], pred[0][2]], \n",
    "               'true_neutral': [pred[1][0], pred[1][1], pred[1][2]], \n",
    "               'true_positive': [pred[2][0], pred[2][1], pred[2][2]]}\n",
    "print (\"{:<15} {:<15} {:<15} {:<15}\".format('','pred_negative','pred_neutral','pred_positive'))\n",
    "for k, v in metric_dict.items():\n",
    "    neg_pred, neu_pred, pos_pred = v\n",
    "    print (\"{:<15} {:<15} {:<15} {:<15}\".format(k, neg_pred, neu_pred, pos_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2fbaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining the Hamming distance and accuracy to one error measure\n",
    "\n",
    "combined_error = (sum(h_dist_cl)**2+(len(acc)-sum(acc))**2) / (len(acc)**2)\n",
    "print(f'Out of {len(acc)} approved images, the combined error for {cluster_type} clustering is {combined_error}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
